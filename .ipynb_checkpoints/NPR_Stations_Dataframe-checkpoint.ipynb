{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPR Stations DataFrame by County\n",
    "=========\n",
    "---------------\n",
    "Create a Dataframe including following columns:\n",
    "* FIPS Code\n",
    "* County Name\n",
    "* State\n",
    "* City\n",
    "* Station Identifier\n",
    "* AM/FM\n",
    "* Frequency\n",
    "* Twitter Account\n",
    "\n",
    "## Import Libraries and configuration files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Station Identifier</th>\n",
       "      <th>AM/FM Number</th>\n",
       "      <th>AM/FM Number.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WBHM</td>\n",
       "      <td>FM</td>\n",
       "      <td>90.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dothan</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WRWA</td>\n",
       "      <td>FM</td>\n",
       "      <td>88.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gadsden</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WSGN</td>\n",
       "      <td>FM</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WJAB</td>\n",
       "      <td>FM</td>\n",
       "      <td>90.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>WLRH</td>\n",
       "      <td>FM</td>\n",
       "      <td>89.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City    State Station Identifier AM/FM Number  AM/FM Number.1\n",
       "0  Birmingham  Alabama               WBHM           FM            90.3\n",
       "1      Dothan  Alabama               WRWA           FM            88.7\n",
       "2     Gadsden  Alabama               WSGN           FM            91.5\n",
       "3  Huntsville  Alabama               WJAB           FM            90.9\n",
       "4  Huntsville  Alabama               WLRH           FM            89.3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr_stations_csv = \"./Resources/nprstations.csv\"\n",
    "npr_stations_df = pd.read_csv(npr_stations_csv)\n",
    "npr_stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to be used during data collection\n",
    "processed = 0\n",
    "notfound = 0\n",
    "\n",
    "#Loop to collect Lat/Long for Agency Names\n",
    "for (idx, row) in npr_stations_df.iterrows():\n",
    "    try:\n",
    "        #Store current row value for each column\n",
    "        city = (row.loc['City'])\n",
    "        state = (row.loc['State'])\n",
    "        #URL from Google APIs where Lat/Long values will be collected\n",
    "        query_url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={city},{state}&key={gkey}\"\n",
    "        #JSON request\n",
    "        response = requests.get(query_url).json()\n",
    "        #Variables to store lat/long values \n",
    "        lat = response[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "        lng = response[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "        #Lat/Long columns created and respective values stored in current row\n",
    "        npr_stations_df.at[idx, 'Latitude'] = lat\n",
    "        npr_stations_df.at[idx, 'Longitude'] = lng\n",
    "        #Print to verify data is processing as expected\n",
    "        print(f\"{city},{state}:{lat},{lng}\")\n",
    "        #Count to validate how many agency names have been processed\n",
    "        processed += 1\n",
    "    #Error handling if some of the agency names are not found\n",
    "    except:\n",
    "        #Print to verify when an agency is not found\n",
    "        print(\"City not found\")\n",
    "        #Count to validate how many agency names were not found\n",
    "        notfound += 1\n",
    "        continue\n",
    "#Print to validate final count for both processed and not found agency names\n",
    "print(f\"{processed} cities were processed. {notfound} cities were not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataFrame to CSV File\n",
    "npr_stations_df.to_csv('npr_stations_by_city.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV and create Dataframe\n",
    "npr_by_county_csv = 'npr_stations_by_city.csv'\n",
    "npr_by_county_df = pd.read_csv(npr_by_county_csv)\n",
    "\n",
    "#URL to get FIPS code\n",
    "url = 'https://geo.fcc.gov/api/census/block/find?'\n",
    "#Variables to be used during data collection\n",
    "processed = 0\n",
    "notfound = 0\n",
    "\n",
    "#Loop to add FIPS code and County name to Dataframe\n",
    "for (idx, row) in npr_by_county_df.iterrows():\n",
    "    try:\n",
    "        #Store current row value for each column\n",
    "        latitude = (row.loc['Latitude'])\n",
    "        longitude = (row.loc['Longitude'])\n",
    "        #URL to collect FIPS code and county name\n",
    "        query_url = f\"{url}&latitude={latitude}&longitude={longitude}&format=json\"\n",
    "        #JSON Request\n",
    "        code = requests.get(query_url).json()\n",
    "        #Variables to store required values (FIPS codes are converted to strings to keep leading zeros in the code)\n",
    "        county_fips = str(code[\"County\"][\"FIPS\"]).zfill(5)\n",
    "        county_name = code[\"County\"][\"name\"]\n",
    "        block_fips = str(code[\"Block\"][\"FIPS\"]).zfill(15)\n",
    "        #New columns created and values collected from JSON stored in current row \n",
    "        npr_by_county_df.at[idx, 'FIPS'] = str(county_fips)\n",
    "        npr_by_county_df.at[idx, 'County Name'] = county_name\n",
    "        npr_by_county_df.at[idx, 'FIPS_block'] = str(block_fips)         \n",
    "        #Count to validate how many records were processed\n",
    "        processed += 1\n",
    "        #Print to verify that information is processed\n",
    "        print(f\"{county_name}:{county_fips}\")\n",
    "    #Error handling if a location is not found\n",
    "    except:\n",
    "        #Print to validate if a location is not found\n",
    "        print(\"Location Not Found\")\n",
    "        #Count to validate how many records were not found\n",
    "        notfound += 1\n",
    "        pass\n",
    "#Print to validate final count for both processed and not found locations\n",
    "print(f\"{processed} locations were processed. {notfound} locations were not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with no FIPS \n",
    "npr_by_county_df = npr_by_county_df[npr_by_county_df['FIPS'].notnull()]\n",
    "\n",
    "#Loop to fill leading zeros on FIPS codes\n",
    "for (idx, row) in npr_by_county_df.iterrows():\n",
    "    fips_z = (row.loc['FIPS'])\n",
    "    fips_block_z = (row.loc['FIPS_block'])\n",
    "    npr_by_county_df.at[idx, 'FIPS'] = str(fips_z).zfill(5)\n",
    "    npr_by_county_df.at[idx, 'FIPS_block'] = str(fips_block_z).zfill(15)\n",
    "\n",
    "#Count duplicate counties\n",
    "npr_by_county_df['duplicate_county'] = npr_by_county_df.groupby('FIPS').count()\n",
    "print(npr_by_county_df['duplicate_county'])\n",
    "    \n",
    "#Reorder dataframe columns\n",
    "npr_by_county_df = npr_by_county_df[[\"FIPS\", \"County Name\", \"City\",  \"State\", \"Latitude\", \"Longitude\", \"Station Identifier\", \"AM/FM Number\", \"AM/FM Number.1\",\"Twitter Account\",\"FIPS_block\"]]\n",
    "#Sort records by FIPS code\n",
    "npr_by_county_df = npr_by_county_df.sort_values(by=['FIPS'])\n",
    "\n",
    "#Show dataframe\n",
    "npr_by_county_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
